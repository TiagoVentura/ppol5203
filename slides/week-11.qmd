---
title: "<span style = 'font-size: 100%;'> PPOL 5203 - Data Science I: Foundations </span>"
subtitle: "<span style = 'font-size: 150%;'> Week 10: Text-As-Data I: Description and Topics </span>"
author: "Professor: Tiago Ventura"
execute: 
  echo: false
  error: true
format:
  revealjs: 
    transition: slide
    background-transition: fade
    code-line-numbers: false
    width: 1600
    height: 1000
    center: true
    slide-number: true
    incremental: false
    chalkboard: 
      buttons: false
    preview-links: auto
    footer: "Data science I: Foundations"
    theme: [simple, custom.scss]
---

## A significant part of how we produce and store data is textual

```{r  echo=FALSE, out.width = "100%", fig.align="center"}
knitr::include_graphics("week1_figs/digital.jpeg") 
```

## Official Documents: Congressional Speeches, Bills, Press Releases, Transcripts, from all over the world!! {visibility="hidden"}

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.brookings.edu/wp-content/uploads/2018/11/RTS10VM4.jpg") 
```

## Social Media {visibility="hidden"}

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("week1_figs/redes.png") 
```

## Online Media: News, Comments, Blogs, etc... {visibility="hidden"}

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://miro.medium.com/v2/resize:fit:1400/1*t2u_0FHoQSpBK8i7j1NOBg.png") 
```

# Modeling Text as Data

## Why do we need models to analyze text data?

**Scalability** and **Dimensionality Reduction**

-   Humans are great at understanding the meaning of a sentence or a document in-depth

-   Computers are better at understanding patterns, classify and describe content across millions of subjects

-   Computational Text analysis augments humans -- does not replace us

-   Statistical models + Powerful Computers allows us to process data at scale and understand common patterns

-   We know "All text models are wrong" -- This is particularly true with text!

## Workflow

-   **Acquire textual data:**

    -   Existing corpora; scraped data; digitized text

-   **Preprocess the data:**

    -   Bag-of-words
    -   Reduce noise, capture signal

## Workflow

-   **Apply method appropriate to research goal:**

    -   Descriptive Analysis:
        -   Count Words, Readability; similarity;
    -   Classify documents into unknown categories
        -   Document clustering
        -   Topic models
    -   Classify documents into known categories
        -   Dictionary methods
        -   Supervised machine learning
        -   Transfer-Learning - use models trained in text for other purposes

## Already did it!

-   **Acquire textual data:**
    -   Existing corpora; scraped data; digitized text

## Today:

-   **Preprocess the data:**

    -   Convert text to numbers.
    -   Use the Bag-of-words assumption.
    -   Reduce noise, capture signal

-   **Apply methods appropriate to research goal:**

    -   Descriptive Analysis:
        -   Count Words, Readability; similarity;
    -   Classify documents into unknown categories
        -   Topic models

## Next Week:

-   Classify documents into known categories
    -   Dictionary methods
    -   Supervised machine learning
    -   Transfer-Learning - use models trained in text for other purposes

# Very quick introdution! Want learn more? Take the Text-as-Data Class in the future.

## From Text-to-Numbers: Representing Texts as Data.

When working with text, we have two critical challenges:

-   Reduce Textual Complexity (Think about every word as a variable. This is huge matrix!!)

    -   Pre-Processing steps

-   Convert unstructured text to numbers \~ that we can feed in to a statistical model

    -   Document Feature Matrix/Bag-of-Words

## Reducing Complexity: Pre-Processing Steps

-   **Tokenize:** break out larger chunks of text in words (unigram) or n-grams

-   **Lowercase:** convert all to lower case

-   **Remove Noise:** stopwords, numbers, punctuation, function words

-   **Stemming:** chops off the ends of words

-   **Lemmatization:** doing things properly with the use of a vocabulary and morphological analysis of words

## Example

::: {.callout-note icon="false"}
## Text

*We use a new dataset containing nearly 50 million historical newspaper pages from 2,700 local US newspapers over the years 1877--1977. We define and discuss a measure of power we develop based on observed word frequencies, and we validate it through a series of analyses. Overall, we find that the relative coverage of political actors and of political offices is a strong indicator of political power for the cases we study*
:::

:::: fragment
::: {.callout-note icon="false"}
## After pre-processing

*use new dataset contain near 50 million historical newspaper pag 2700 local u newspaper year 18771977 define discus measure power develop bas observ word frequenc validate ser analys overall find relat coverage political actor political offic strong indicator political power cas study*
:::
::::

## Text Representation

-   Represent text as an **unordered** set of tokens in a document. [Bag-of-Words Assumption]{.red}

-   While order is ignored, frequency matters!

-   To represent documents as numbers, we will use the [vector space model representation]{.red}:

    -   A document $D_i$ is represented as a collection of features $W$ (words, tokens, n-grams..)

    -   Each feature $w_i$ can be placed in a real line, then a document $D_i$ is a vector with $W$ dimensions

::: fragment
Imagine the sentence below: *"If that is a joke, I love it. If not, can't wait to unpack that with you later."*

-   **Sorted Vocabulary** =(a, can't, i, if, is, it, joke, later, love, not, that, to, unpack, wait, with, you")

-   **Feature Representation** = (1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1)

-   Features will typically be the n-gram (mostly unigram) [frequencies]{.red} of the tokens in the document, or some [function]{.red} of those frequencies
:::

::: fragment
Each document becomes a numerical vector (vector space model)

-   stacking these vectors will give you our workhose representation for text: [**Document Feature Matrix**]{.red}
:::

## Visualizing Vector Space Model

::: {.callout-note icon="false"}
## Documents

Document 1 = "yes yes yes no no no"

Document 2 = "yes yes yes yes yes yes"
:::

```{r echo=FALSE, fig.align="center",fig.width=8}
# Load necessary libraries
pacman::p_load(ggplot, tidyverse)

# Define a simple vocabulary of two words

# Sample texts using only words from the vocabulary
document1 <- c("yes", "yes", "yes", "no", "no", "no")
document2 <- c("no", "no", "no", "no", "no", "no")

# Convert the documents to a dataframe to a data frame for ggplot
df <- tibble(document1, document2) %>% 
       pivot_longer(cols=contains("document"), names_to = "document", values_to = "word") %>%
       group_by(document, word) %>%
       count() %>%
       pivot_wider(names_from=word, values_from=n, values_fill = 0)

# Plot the documents in 2D space using ggplot
ggplot(df, aes(x = no, y = yes, label = document)) +
  geom_label(nudge_y =.5) +
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), 
               arrow = arrow(), 
               size=1, color="navy") +
  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  
               arrow = arrow(), 
               size=1, color="navy") +
  xlim(0, 7) +
  xlab("Frequency of 'yes'") +
  ylab("Frequency of 'no'") +
  ggtitle("Vector Representation of Texts") +
  theme_minimal()
```

## From Text to Numbers: Bag-of-Words & Document-Feature Matrix

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.mzes.uni-mannheim.de/socialsciencedatalab/article/advancing-text-mining/figures/dfm.png") 
```

## Text-as-Data methods today

-   Comparing/Describing Documents

-   Topic Models: Finding hidden structures in a corpus

## Comparing Documents: How \`far' is document a from document b?

Using the vector space, we can use notions of geometry to build well-defined [comparison/similarity]{.red} measures between the documents.

:::: incremental
-   [in multiple dimensions!!]{.red}

::: fragment
```{r echo=FALSE, fig.align="center",fig.width=8}

# Plot the documents in 2D space using ggplot
ggplot(df, aes(x = no, y = yes, label = document)) +
  geom_point(size=2) +
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), 
               arrow = arrow(), 
               size=1, color="navy", alpha=.2) +
  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  
               arrow = arrow(), 
               size=1, color="navy", alpha=.3) +
 geom_segment(aes(x = 3, y = 3, xend = 6, yend = 0),  
               linetype=2, 
               size=1, color="tomato", alpha=1) +
annotate(geom="label", x=5, y=1, label="Distance",
              color="black")  +
  xlim(0, 7) +
  xlab("Frequency of 'yes'") +
  ylab("Frequency of 'no'") +
  ggtitle("Euclidean Distance Between Documents") +
  theme_minimal()
```
:::
::::

```{=html}
<!-- ## Properties for Similarity Metrics

For vectors i and j, with distance $s_{ij}$:

- 1. no negative distances: $s_{ij}  \ge 0$

- 2. when distance between documents is zero ~ documents are identical

- 3. distance between documents is symmetric: sij = sji

- 4 measures satisfy triangle inequality. sik > sij + sjk 
-->
```

## Euclidean Distance

The *ordinary*, *straight line* distance between two points in space. Using document vectors $y_a$ and $y_b$ with $j$ dimensions

::: {.callout-note icon="false"}
## Euclidean Distance

$$
||y_a - y_b|| = \sqrt{\sum^{j}(y_{aj} - y_{bj})^2}
$$
:::

#### Can be performed for any number of features J \~ has nice mathematical properties

::: notes
no negative distances: sij   0 2 distance between documents is zero () documents are identical 3 distance between documents is symmetric: sij = sji 4 measures satisfy triangle inequality. sik   sij + sjk
:::

## Euclidean Distance, example

::: {.callout-note icon="false"}
## Euclidean Distance

$$
||y_a - y_b|| = \sqrt{\sum^{j}(y_{aj} - y_{bj})^2}
$$
:::

-   $y_a$ = \[0, 2.51, 3.6, 0\] and $y_b$ = \[0, 2.3, 3.1, 9.2\]

-   $\sum_{j=1}^j (y_a - y_b)^2$ = $(0-0)^2 + (2.51-2.3)^2 + (3.6-3.1)^2 + (9-0)^2$ = $84.9341$

-   $\sqrt{\sum_{j=1}^j (y_a - y_b)^2}$ = 9.21

```{r  echo=FALSE, eval=FALSE}
a = c(0, 2.51, 3.6, 0)
b = c(0, 2.3, 3.1, 9.2)
sqrt(sum((a-b)^2))

```

## Exercise

::: {.callout-note icon="false"}
## Documents, W=3 {yes, no}

Document 1 = "yes yes yes no no no" (3, 3)

Document 2 = "yes yes yes yes yes yes" (6,0)

Document 3= "yes yes yes no no no yes yes yes no no no yes yes yes no no no yes yes yes no no no" (12, 12)
:::

::::: columns
::: {.column width="50%"}
```{r echo=FALSE, fig.align="center",fig.width=8}
# Load necessary libraries
pacman::p_load(tidyverse)

# Define a simple vocabulary of two words

# Sample texts using only words from the vocabulary
document1 <- tibble(yes=3, no=3)
document2 <- tibble(yes=6, no=0)
document3 <- tibble(yes=12, no=12)

# Convert the documents to a dataframe to a data frame for ggplot
df <- bind_rows(document1, document2, document3) %>%
      mutate(document=c("Doc A", "Doc B", "Doc C"))



# Plot the documents in 2D space using ggplot
ggplot(df, aes(x = yes, y = no, label = document)) +
  geom_label(nudge_y =.5) +
  geom_point(shape=21, fill="tomato", size=4) +
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), 
               arrow = arrow(), 
               size=1, color="navy") +
  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  
               arrow = arrow(), 
               size=1, color="navy") +
  geom_segment(aes(x = 0, y = 0, xend = 12, yend = 12),  
               arrow = arrow(), 
               size=1, color="navy") +
  xlab("Frequency of 'yes'") +
  ylab("Frequency of 'no'") +
  ggtitle("Vector Representation of Texts") +
  theme_minimal()

```
:::

::: {.column width="50%"}
<br><br>

-   Which documents will the euclidean distance place closer together?
-   Does it look like a good measure for similarity?
    -   Doc C = Doc A \* 3
:::
:::::

## Cosine Similarity

Euclidean distance rewards [magnitude]{.red}, rather than [direction]{.red}

$$
\text{cosine similarity}(\mathbf{y_a}, \mathbf{y_b}) = \frac{\mathbf{y_a} \cdot \mathbf{y_b}}{\|\mathbf{y_a}\| \|\mathbf{y_b}\|}
$$

-   $U \cdot V$ \~ dot product between vectors

    -   it is a projection of one vector into another
    -   Works as a measure of similarity
    -   But rewards magnitude

-   $||\mathbf{U}||$ \~ vector magnitude, length \~ $\sqrt{\sum{u_{i}^2}}$

    -   normalizes a vector projection of documents' by their lengths

-   cosine similarity captures some notion of relative *direction* controlling for different magnitudes.

## Cosine Similarity

Cosine function has a range between -1 and 1.

-   Consider: cos (0) = 1, cos (90) = 0, cos (180) = -1

```{r}
# Load necessary libraries
pacman::p_load(ggplot, tidyverse)

# Define a simple vocabulary of two words

# Sample texts using only words from the vocabulary
document1 <- tibble(yes=3, no=3)
document2 <- tibble(yes=6, no=0)
document3 <- tibble(yes=12, no=12)

# Convert the documents to a dataframe to a data frame for ggplot
df <- bind_rows(document1, document2, document3) %>%
      mutate(document=c("Doc A", "Doc B", "Doc C"))

# Function to calculate the angle between two vectors
calculate_angle <- function(x1, y1, x2, y2) {
  dot_product <- x1 * x2 + y1 * y2
  magnitudes <- sqrt(x1^2 + y1^2) * sqrt(x2^2 + y2^2)
  angle <- acos(dot_product / magnitudes)
  return(angle * (180 / pi)) # Convert from radians to degrees
}

# Calculate angles between vectors
angle1_2 <- calculate_angle(3, 3, 6, 0)
angle1_3 <- calculate_angle(3, 3, 12, 12)
angle2_3 <- calculate_angle(6, 0, 12, 12)

# Plot the documents in 2D space using ggplot
ggplot(df, aes(x = yes, y = no, label = document)) +
  geom_label(nudge_y =.5) +
  geom_point(shape=21, fill="tomato", size=4) +
  geom_segment(aes(x = 0, y = 0, xend = 3, yend = 3), 
               arrow = arrow(), 
               size=1, color="navy", alpha=0.05) +
  geom_segment(aes(x = 0, y = 0, xend = 6, yend = 0),  
               arrow = arrow(), 
               size=1, color="navy", alpha=.05) +
  geom_segment(aes(x = 0, y = 0, xend = 12, yend = 12),  
               arrow = arrow(), 
               size=1, color="navy", alpha=.05) +
geom_curve(aes(x =1.5, y = 1.5, xend = 2, yend = 0), curvature = -0.5, 
           linetype="dashed", color="tomato") +
annotate("text", x = 2.5, y = 1.0, label = paste("θ"), size=5) +
  xlab("Frequency of 'yes'") +
  ylab("Frequency of 'no'") +
  ggtitle("Vector Representation of Texts with Angles") +
  theme_minimal()

```

# Topic Models: LDA Approach

## Topic Models: Intuition

-   **Problem:** Once we start with docs, we do not know what topics exist, what words belong to which topics, what topic proportions each document has

-   **Probabilistic Model:** Make assumptions about how language work, and how topics emerge. Then check if this matches with word co-occurrence in each document

-   **Documents:** formed from probability distribution of topics

    -   [a speech can be 40% about trade, 30% about sports, 10% about health, and 20% spread across topics you don't think make much sense]{.midgray}

-   **Topics:** formed from probability distribution over words

    -   [the topic health will have words like hospital, clinic, dr., sick, cancer]{.midgray}

## Blei, 2012,

```{r  echo=FALSE, out.width = "80%", fig.align="center"}
knitr::include_graphics("https://www.analyticsvidhya.com/wp-content/uploads/2016/08/Modeling1.png") 
```

## Intuition: Language Model

-   **Step 1: For each [document]{.red}:**

    -   Randomly choose a [distribution]{.red} over topics. That is, choose one of many multinomial distributions, each which mixes the topics in different proportions.

-   **Step 2: For each topic**

    -   Randomly choose a **distribution** of words, also from one of many multinomial distributions., each with mixes words in different proportions

-   **Step 3: Then, for every [word]{.red} in the document**

    -   Randomly choose a [topic]{.red} from the distribution over topics from step 1.
    -   Randomly choose a [word]{.red} from the distribution over the vocabulary that the topic implies.

## Inference: How to estimate all these parameters?

**Inference:** Use the observed data, the words, to make an inference about the latent parameters: the $\beta$s, and the $\theta$s.

**Complicate Math:** Using the observed data, the words, we can estimate latent parameters. We start with the joint distribution implied by our language model (Blei, 2012):

$$
p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D})= \prod_{K}^{i=1}p(\beta_i)\prod_{D}^{d=1}p(\theta_d)(\prod_{N}^{n=1}p(z_{d,n}|\theta_d)p(w_{d,n}|\beta_{1:K},z_{d,n})
$$

To get to this conditional:

$$
p(\beta_{1:K}, \theta_{1:D}, z_{1:D}|w_{1:D})=\frac{p(\beta_{1:K}, \theta_{1:D}, z_{1:D}, w_{1:D})}{p(w_{1:D})}
$$

**Estimation:** Use Gibbs Sampling (Bayesian Stats) to reverse-engineer this

-   Start at random for all parameters, compare with word co-occurrence within docs, and update

-   Why it works: The model wants a topic to contain as few words as possible, but a document to contain as few topics as possible. This tension is what makes the model work.

## Show me results!

```{r echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("topic_matrices.png") 
```

## Many different types of topic models

-   **Structural topic model:** allow (1) topic prevalence, (2) topic content to vary as a function of document-level covariates (e.g., how do topics vary over time or documents produced in 1990 talk about something differently than documents produced in 2020?); implemented in stm in R (Roberts, Stewart, Tingley, Benoit)

-   **Correlated topic model:** way to explore between-topic relationships (Blei and Lafferty, 2017); implemented in topicmodels in R; possibly somewhere in Python as well!

-   **Keyword-assisted topic model:** seed topic model with keywords to try to increase the face validity of topics to what you're trying to measure; implemented in keyATM in R (Eshima, Imai, Sasaki, 2019)

-   **BertTopic:** BERTopic is a topic modeling technique that leverages transformers and TF-IDF to create dense clusters of words.

# Notebooks
